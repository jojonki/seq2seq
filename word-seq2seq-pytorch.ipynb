{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original: http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N 5424 == 5424\n",
      "n_input_vocab 1730\n",
      "n_target_vocab 3040\n",
      "max enc len 6\n",
      "max dec len 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonki/anaconda3/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "def word_tokenize(sent):\n",
    "    return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n",
    "\n",
    "n_samples = 10000\n",
    "data_path = 'fra-eng/fra.txt' # you have to download dataset from http://www.manythings.org/anki/\n",
    "max_sentence_len = 6\n",
    "UNK = '<UNK>'\n",
    "SOS = '<SOS>'\n",
    "EOS = '<EOS>'\n",
    "input_texts, target_texts = [], []\n",
    "input_vocab, target_vocab = set(), set()\n",
    "lines = open(data_path).read().split('\\n')\n",
    "for line in lines[:min(n_samples, len(lines) -1)]:\n",
    "    in_txt, tg_txt = line.split('\\t', 1)\n",
    "    in_txt = word_tokenize(in_txt) + [EOS]\n",
    "    tg_txt = word_tokenize(tg_txt) + [EOS]\n",
    "    if len(in_txt) > max_sentence_len or len(tg_txt) > max_sentence_len: continue # skip difficult sentence\n",
    "    input_texts.append(in_txt)\n",
    "    target_texts.append(tg_txt)\n",
    "    for w in in_txt:\n",
    "        if w not in input_vocab:\n",
    "            input_vocab.add(w)\n",
    "    for w in tg_txt:\n",
    "        if w not in target_vocab:\n",
    "            target_vocab.add(w)\n",
    "\n",
    "input_vocab = [UNK, SOS, EOS] + sorted(list(input_vocab))\n",
    "target_vocab = [UNK, SOS, EOS] + sorted(list(target_vocab)) \n",
    "n_input_vocab = len(input_vocab)\n",
    "n_target_vocab = len(target_vocab)\n",
    "max_encoder_seq_len = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_len = max([len(txt) for txt in target_texts])\n",
    "print('N', len(input_texts), '==', len(target_texts))\n",
    "print('n_input_vocab', n_input_vocab)\n",
    "print('n_target_vocab', n_target_vocab)\n",
    "print('max enc len', max_encoder_seq_len)\n",
    "print('max dec len', max_decoder_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ['Go', '.', '<EOS>']\n",
      "target: ['Va', '!', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "input_w2i = {w:i for i,w in enumerate(input_vocab)}\n",
    "input_i2w = {i:w for i,w in enumerate(input_vocab)}\n",
    "target_w2i = {w:i for i,w in enumerate(target_vocab)}\n",
    "target_i2w = {i:w for i,w in enumerate(target_vocab)}\n",
    "print('input:', input_texts[0])\n",
    "print('target:', target_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodeRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncodeRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1) # torch.Size([1, 1, 100])\n",
    "        \n",
    "        output = embedded\n",
    "        for _ in range(self.n_layers):\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, output_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input, hidden, dummy1, dummy2):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        for _ in range(self.n_layers):\n",
    "            output = F.relu(output)\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output[0])\n",
    "        output = nn.LogSoftmax(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda: return result.cuda()\n",
    "        else: return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1, max_length=10):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size*2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size*2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_ouput, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        attn_weights = F.softmax(\n",
    "                            self.attn(torch.cat((embedded[0], hidden[0]), 1))\n",
    "        )\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0)) # Performs a batch matrix-matrix product of matrices stored in batch1 and batch2.\n",
    "        \n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        \n",
    "        for _ in range(self.n_layers):\n",
    "            output = F.relu(output)\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "            \n",
    "        output = F.log_softmax(self.out(output[0]))\n",
    "        return output, hidden, attn_weights\n",
    "    \n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda: return result.cuda()\n",
    "        else: return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_var, target_var, encoder, decoder, enc_optim, dec_optim, criterion, max_length):\n",
    "    enc_optim.zero_grad()\n",
    "    dec_optim.zero_grad()\n",
    "    \n",
    "    input_length = input_var.size()[0]\n",
    "    target_length = target_var.size()[0]\n",
    "    \n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    if use_cuda: encoder_outputs = encoder_outputs.cuda()\n",
    "    \n",
    "    loss = 0\n",
    "    encoder_hiden = encoder.initHidden()\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hiden = encoder(input_var[ei], encoder_hiden)\n",
    "        encoder_outputs[ei] = encoder_output[0][0]\n",
    "        \n",
    "    decoder_input = Variable(torch.LongTensor([[target_w2i[SOS]]]))\n",
    "    if use_cuda: decoder_input = decoder_input.cuda()\n",
    "\n",
    "    decoder_hidden = encoder_hiden\n",
    "    use_teacher_forcing = True if random.random() < 0.5 else False\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_output, encoder_outputs\n",
    "        )\n",
    "        if use_teacher_forcing:\n",
    "            loss += criterion(decoder_output, target_var[di])\n",
    "            decoder_input = target_var[di] # teacher_forcing\n",
    "        else:\n",
    "            top_val, top_ind = decoder_output.data.topk(1)\n",
    "            ni = top_ind[0][0]\n",
    "\n",
    "            decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "            if use_cuda: decoder_input = decoder_input.cuda()\n",
    "\n",
    "            loss += criterion(decoder_output, target_var[di])\n",
    "            if ni == target_w2i[EOS]:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "    enc_optim.step()\n",
    "    dec_optim.step()\n",
    "    \n",
    "    return loss.data[0] / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden size (1, 1, 3040), got (1, 1, 100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-be4d3453f1da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# run(EncodeRNN(n_input_vocab, hidden_size), AttnDecoderRNN(hidden_size, n_target_vocab))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEncodeRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_input_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_target_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'finished training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-be4d3453f1da>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(encoder, decoder)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# run(EncodeRNN(n_input_vocab, hidden_size), AttnDecoderRNN(hidden_size, n_target_vocab))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-be4d3453f1da>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         loss = train(input_variable, target_variable, encoder, decoder, \n\u001b[0;32m---> 24\u001b[0;31m                      encoder_optimizer, decoder_optimizer, criterion, 10)\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mplot_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-a88e3309e8e8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_var, target_var, encoder, decoder, enc_optim, dec_optim, criterion, max_length)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         decoder_output, decoder_hidden, decoder_attention = decoder(\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         )\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_teacher_forcing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-4249b6c82e28>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden, dummy1, dummy2)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         )\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutogradRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36m_do_forward\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mflat_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_iter_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0mflat_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNestedIOFunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mflat_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0mnested_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mnested_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mnested_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_map_variable_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_extended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward_extended\u001b[0;34m(self, input, weight, hx)\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mhy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/backends/cudnn/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(fn, input, hx, weight, output, hy)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             raise RuntimeError('Expected hidden size {}, got {}'.format(\n\u001b[0;32m--> 266\u001b[0;31m                 hidden_size, tuple(hx.size())))\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             raise RuntimeError('Expected cell size {}, got {}'.format(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected hidden size (1, 1, 3040), got (1, 1, 100)"
     ]
    }
   ],
   "source": [
    "def variableFromSentence(sentence, w2i):\n",
    "    index_vector = [w2i[w] for w in sentence]\n",
    "    index_vector += [w2i[EOS]]\n",
    "    result = Variable(torch.LongTensor(index_vector).view(-1, 1))\n",
    "    if use_cuda: result = result.cuda()\n",
    "    return result\n",
    "\n",
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    input_variables = [variableFromSentence(random.choice(input_texts), input_w2i) for i in range(n_iters)]\n",
    "    target_variables = [variableFromSentence(random.choice(target_texts), target_w2i) for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        input_variable = input_variables[iter - 1]\n",
    "        target_variable = target_variables[iter - 1]\n",
    "\n",
    "        loss = train(input_variable, target_variable, encoder, decoder, \n",
    "                     encoder_optimizer, decoder_optimizer, criterion, 10)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('iter_count={:d} ({:2f}%), loss={:4f}'.format(iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "    \n",
    "hidden_size = 100\n",
    "def run(encoder, decoder):\n",
    "    if use_cuda:\n",
    "        encoder = encoder.cuda()\n",
    "        decoder = decoder.cuda()\n",
    "    trainIters(encoder, decoder, 1000)\n",
    "\n",
    "# run(EncodeRNN(n_input_vocab, hidden_size), AttnDecoderRNN(hidden_size, n_target_vocab))\n",
    "run(EncodeRNN(n_input_vocab, hidden_size), DecoderRNN(hidden_size, n_target_vocab))\n",
    "print('finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<UNK>',\n",
       " '<SOS>',\n",
       " '<EOS>',\n",
       " '!',\n",
       " \"'\",\n",
       " ',',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '?',\n",
       " 'A',\n",
       " 'Abandon',\n",
       " 'About',\n",
       " 'Act',\n",
       " 'Adjust',\n",
       " 'After',\n",
       " 'Aim',\n",
       " 'Air',\n",
       " 'All',\n",
       " 'Am',\n",
       " 'American',\n",
       " 'Answer',\n",
       " 'Any',\n",
       " 'Anything',\n",
       " 'Apples',\n",
       " 'Arabs',\n",
       " 'Are',\n",
       " 'As',\n",
       " 'Asian',\n",
       " 'Ask',\n",
       " 'Attack',\n",
       " 'Awesome',\n",
       " 'Azerbaijani',\n",
       " 'Bach',\n",
       " 'Back',\n",
       " 'Balls',\n",
       " 'Be',\n",
       " 'Beat',\n",
       " 'Beats',\n",
       " 'Behave',\n",
       " 'Beware',\n",
       " 'Birds',\n",
       " 'Bless',\n",
       " 'Blow',\n",
       " 'Boats',\n",
       " 'Boston',\n",
       " 'Bottoms',\n",
       " 'Boys',\n",
       " 'Break',\n",
       " 'Bring',\n",
       " 'British',\n",
       " 'CDs',\n",
       " 'CPR',\n",
       " 'Call',\n",
       " 'Calm',\n",
       " 'Can',\n",
       " 'Canadian',\n",
       " 'Carry',\n",
       " 'Catch',\n",
       " 'Cats',\n",
       " 'Change',\n",
       " 'Check',\n",
       " 'Cheer',\n",
       " 'Cheers',\n",
       " 'Chinese',\n",
       " 'Choose',\n",
       " 'Christmas',\n",
       " 'Clean',\n",
       " 'Close',\n",
       " 'Come',\n",
       " 'Congratulations',\n",
       " 'Cook',\n",
       " 'Cool',\n",
       " 'Could',\n",
       " 'Count',\n",
       " 'Cover',\n",
       " 'Cuff',\n",
       " 'Cut',\n",
       " 'DVD',\n",
       " 'Dance',\n",
       " 'Did',\n",
       " 'Dig',\n",
       " 'Dinner',\n",
       " 'Do',\n",
       " 'Does',\n",
       " 'Dogs',\n",
       " 'Don',\n",
       " 'Draw',\n",
       " 'Drink',\n",
       " 'Drive',\n",
       " 'Drop',\n",
       " 'Dry',\n",
       " 'Easy',\n",
       " 'Eat',\n",
       " 'Empty',\n",
       " 'English',\n",
       " 'Enjoy',\n",
       " 'Enter',\n",
       " 'Everybody',\n",
       " 'Everyone',\n",
       " 'Excuse',\n",
       " 'Fantastic',\n",
       " 'Feed',\n",
       " 'Feel',\n",
       " 'Fill',\n",
       " 'Find',\n",
       " 'Finnish',\n",
       " 'Fire',\n",
       " 'Firefox',\n",
       " 'Fishing',\n",
       " 'Flip',\n",
       " 'Fold',\n",
       " 'Follow',\n",
       " 'Forget',\n",
       " 'Forgive',\n",
       " 'Form',\n",
       " 'French',\n",
       " 'Get',\n",
       " 'Ghosts',\n",
       " 'Girls',\n",
       " 'Give',\n",
       " 'Go',\n",
       " 'God',\n",
       " 'Good',\n",
       " 'Goodbye',\n",
       " 'Got',\n",
       " 'Grab',\n",
       " 'Guess',\n",
       " 'Guys',\n",
       " 'Hand',\n",
       " 'Hands',\n",
       " 'Hang',\n",
       " 'Happy',\n",
       " 'Has',\n",
       " 'Have',\n",
       " 'He',\n",
       " 'Hello',\n",
       " 'Help',\n",
       " 'Her',\n",
       " 'Here',\n",
       " 'Hey',\n",
       " 'Hi',\n",
       " 'His',\n",
       " 'Hold',\n",
       " 'Hop',\n",
       " 'Horses',\n",
       " 'How',\n",
       " 'Humor',\n",
       " 'Hungarian',\n",
       " 'Hurry',\n",
       " 'Hyogo',\n",
       " 'I',\n",
       " 'If',\n",
       " 'Ignore',\n",
       " 'Iron',\n",
       " 'Is',\n",
       " 'It',\n",
       " 'Italian',\n",
       " 'Japan',\n",
       " 'Japanese',\n",
       " 'Join',\n",
       " 'Jump',\n",
       " 'Just',\n",
       " 'Keep',\n",
       " 'Kids',\n",
       " 'Kiss',\n",
       " 'Kobe',\n",
       " 'Korean',\n",
       " 'Lead',\n",
       " 'Learn',\n",
       " 'Leave',\n",
       " 'Let',\n",
       " 'Lie',\n",
       " 'Life',\n",
       " 'Lighten',\n",
       " 'Listen',\n",
       " 'Live',\n",
       " 'Lock',\n",
       " 'London',\n",
       " 'Look',\n",
       " 'Loosen',\n",
       " 'Love',\n",
       " 'Luck',\n",
       " 'Lunch',\n",
       " 'Make',\n",
       " 'Mama',\n",
       " 'Many',\n",
       " 'Marry',\n",
       " 'Mary',\n",
       " 'May',\n",
       " 'Me',\n",
       " 'Memorize',\n",
       " 'Men',\n",
       " 'Milan',\n",
       " 'Monday',\n",
       " 'Mondays',\n",
       " 'Muslim',\n",
       " 'Must',\n",
       " 'My',\n",
       " 'Never',\n",
       " 'New',\n",
       " 'Nice',\n",
       " 'No',\n",
       " 'Nobody',\n",
       " 'Now',\n",
       " 'OK',\n",
       " 'Of',\n",
       " 'Oh',\n",
       " 'Once',\n",
       " 'Only',\n",
       " 'Open',\n",
       " 'Our',\n",
       " 'Out',\n",
       " 'Pack',\n",
       " 'Pardon',\n",
       " 'Paris',\n",
       " 'Perfect',\n",
       " 'Pick',\n",
       " 'Plants',\n",
       " 'Play',\n",
       " 'Please',\n",
       " 'Prices',\n",
       " 'Pull',\n",
       " 'Put',\n",
       " 'Quit',\n",
       " 'Read',\n",
       " 'Really',\n",
       " 'Red',\n",
       " 'Release',\n",
       " 'Rest',\n",
       " 'Ring',\n",
       " 'Run',\n",
       " 'Saturday',\n",
       " 'Save',\n",
       " 'Say',\n",
       " 'Science',\n",
       " 'Seal',\n",
       " 'See',\n",
       " 'Send',\n",
       " 'Seriously',\n",
       " 'Set',\n",
       " 'Shadow',\n",
       " 'Shall',\n",
       " 'Shame',\n",
       " 'She',\n",
       " 'Should',\n",
       " 'Show',\n",
       " 'Shut',\n",
       " 'Sign',\n",
       " 'Sit',\n",
       " 'Sleep',\n",
       " 'Slow',\n",
       " 'Smoke',\n",
       " 'Smoking',\n",
       " 'Snap',\n",
       " 'So',\n",
       " 'Speak',\n",
       " 'Stand',\n",
       " 'Start',\n",
       " 'Stay',\n",
       " 'Step',\n",
       " 'Stir',\n",
       " 'Stop',\n",
       " 'Straighten',\n",
       " 'Study',\n",
       " 'Stuff',\n",
       " 'Sugar',\n",
       " 'Suit',\n",
       " 'Sundays',\n",
       " 'Sweet',\n",
       " 'Swiss',\n",
       " 'TV',\n",
       " 'Take',\n",
       " 'Talk',\n",
       " 'Tell',\n",
       " 'Terrific',\n",
       " 'Thank',\n",
       " 'Thanks',\n",
       " 'That',\n",
       " 'The',\n",
       " 'Their',\n",
       " 'There',\n",
       " 'These',\n",
       " 'They',\n",
       " 'Think',\n",
       " 'This',\n",
       " 'Those',\n",
       " 'Throw',\n",
       " 'Tie',\n",
       " 'Time',\n",
       " 'Tokyo',\n",
       " 'Tom',\n",
       " 'Too',\n",
       " 'Tough',\n",
       " 'Toyota',\n",
       " 'Treat',\n",
       " 'Trust',\n",
       " 'Try',\n",
       " 'Turn',\n",
       " 'Unbelievable',\n",
       " 'Use',\n",
       " 'Wait',\n",
       " 'Wake',\n",
       " 'Walk',\n",
       " 'War',\n",
       " 'Warn',\n",
       " 'Was',\n",
       " 'Wash',\n",
       " 'Watch',\n",
       " 'We',\n",
       " 'Wednesday',\n",
       " 'Welcome',\n",
       " 'Well',\n",
       " 'Were',\n",
       " 'What',\n",
       " 'When',\n",
       " 'Where',\n",
       " 'Which',\n",
       " 'Who',\n",
       " 'Why',\n",
       " 'Will',\n",
       " 'Wind',\n",
       " 'Wish',\n",
       " 'Wonderful',\n",
       " 'Work',\n",
       " 'Wow',\n",
       " 'Write',\n",
       " 'Year',\n",
       " 'Years',\n",
       " 'You',\n",
       " 'Your',\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'absent',\n",
       " 'absurd',\n",
       " 'accept',\n",
       " 'act',\n",
       " 'active',\n",
       " 'acts',\n",
       " 'adaptable',\n",
       " 'addicted',\n",
       " 'admire',\n",
       " 'adopted',\n",
       " 'adorable',\n",
       " 'adore',\n",
       " 'adores',\n",
       " 'adults',\n",
       " 'adventurous',\n",
       " 'advice',\n",
       " 'afraid',\n",
       " 'after',\n",
       " 'afternoon',\n",
       " 'again',\n",
       " 'age',\n",
       " 'ahead',\n",
       " 'airs',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'already',\n",
       " 'alright',\n",
       " 'also',\n",
       " 'always',\n",
       " 'am',\n",
       " 'amazing',\n",
       " 'ambitious',\n",
       " 'amuse',\n",
       " 'amused',\n",
       " 'amusing',\n",
       " 'an',\n",
       " 'and',\n",
       " 'angry',\n",
       " 'annoying',\n",
       " 'answer',\n",
       " 'answers',\n",
       " 'anxious',\n",
       " 'any',\n",
       " 'anyone',\n",
       " 'anyway',\n",
       " 'apart',\n",
       " 'apologize',\n",
       " 'apologized',\n",
       " 'appeared',\n",
       " 'appreciate',\n",
       " 'approve',\n",
       " 'are',\n",
       " 'arguing',\n",
       " 'armed',\n",
       " 'around',\n",
       " 'arrived',\n",
       " 'artist',\n",
       " 'as',\n",
       " 'ashamed',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asleep',\n",
       " 'assume',\n",
       " 'astonished',\n",
       " 'at',\n",
       " 'attack',\n",
       " 'attend',\n",
       " 'attentive',\n",
       " 'available',\n",
       " 'avoids',\n",
       " 'awake',\n",
       " 'away',\n",
       " 'awesome',\n",
       " 'awful',\n",
       " 'awkward',\n",
       " 'babies',\n",
       " 'baby',\n",
       " 'bachelor',\n",
       " 'back',\n",
       " 'bad',\n",
       " 'baffled',\n",
       " 'bag',\n",
       " 'bags',\n",
       " 'baking',\n",
       " 'bald',\n",
       " 'ball',\n",
       " 'bankrupt',\n",
       " 'bark',\n",
       " 'barked',\n",
       " 'bath',\n",
       " 'be',\n",
       " 'beans',\n",
       " 'beard',\n",
       " 'beaten',\n",
       " 'beautiful',\n",
       " 'became',\n",
       " 'bed',\n",
       " 'beer',\n",
       " 'beg',\n",
       " 'began',\n",
       " 'begin',\n",
       " 'begun',\n",
       " 'behave',\n",
       " 'believe',\n",
       " 'believed',\n",
       " 'bell',\n",
       " 'best',\n",
       " 'bet',\n",
       " 'betrayed',\n",
       " 'better',\n",
       " 'biased',\n",
       " 'big',\n",
       " 'bike',\n",
       " 'bird',\n",
       " 'birds',\n",
       " 'birthday',\n",
       " 'bit',\n",
       " 'bite',\n",
       " 'bites',\n",
       " 'black',\n",
       " 'bled',\n",
       " 'bless',\n",
       " 'blessed',\n",
       " 'blind',\n",
       " 'blood',\n",
       " 'blue',\n",
       " 'blushed',\n",
       " 'boat',\n",
       " 'book',\n",
       " 'books',\n",
       " 'bore',\n",
       " 'bored',\n",
       " 'boring',\n",
       " 'born',\n",
       " 'boss',\n",
       " 'bossy',\n",
       " 'both',\n",
       " 'bother',\n",
       " 'bottom',\n",
       " 'bought',\n",
       " 'bounce',\n",
       " 'box',\n",
       " 'boy',\n",
       " 'boys',\n",
       " 'braces',\n",
       " 'brave',\n",
       " 'bread',\n",
       " 'break',\n",
       " 'breath',\n",
       " 'bribed',\n",
       " 'bright',\n",
       " 'broke',\n",
       " 'broken',\n",
       " 'brothers',\n",
       " 'bulky',\n",
       " 'bummer',\n",
       " 'bunch',\n",
       " 'burn',\n",
       " 'burned',\n",
       " 'bus',\n",
       " 'business',\n",
       " 'busy',\n",
       " 'buy',\n",
       " 'buying',\n",
       " 'by',\n",
       " 'cake',\n",
       " 'call',\n",
       " 'called',\n",
       " 'callous',\n",
       " 'calm',\n",
       " 'came',\n",
       " 'camera',\n",
       " 'can',\n",
       " 'cancel',\n",
       " 'canceled',\n",
       " 'candy',\n",
       " 'capsized',\n",
       " 'car',\n",
       " 'card',\n",
       " 'care',\n",
       " 'careful',\n",
       " 'carefully',\n",
       " 'careless',\n",
       " 'cars',\n",
       " 'cash',\n",
       " 'cat',\n",
       " 'cats',\n",
       " 'caught',\n",
       " 'celebrating',\n",
       " 'certain',\n",
       " 'chair',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'chat',\n",
       " 'cheat',\n",
       " 'cheated',\n",
       " 'cheats',\n",
       " 'check',\n",
       " 'checked',\n",
       " 'cheered',\n",
       " 'children',\n",
       " 'choice',\n",
       " 'choked',\n",
       " 'chubby',\n",
       " 'chuckled',\n",
       " 'circle',\n",
       " 'class',\n",
       " 'clean',\n",
       " 'clear',\n",
       " 'clearly',\n",
       " 'clever',\n",
       " 'clock',\n",
       " 'close',\n",
       " 'closed',\n",
       " 'closely',\n",
       " 'cloudy',\n",
       " 'clues',\n",
       " 'coat',\n",
       " 'coffee',\n",
       " 'coin',\n",
       " 'cold',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'coming',\n",
       " 'comment',\n",
       " 'complain',\n",
       " 'complex',\n",
       " 'concerned',\n",
       " 'confessed',\n",
       " 'confident',\n",
       " 'confused',\n",
       " 'contagious',\n",
       " 'content',\n",
       " 'contented',\n",
       " 'continue',\n",
       " 'cook',\n",
       " 'cookie',\n",
       " 'cooking',\n",
       " 'cooks',\n",
       " 'cool',\n",
       " 'cooperate',\n",
       " 'cop',\n",
       " 'cops',\n",
       " 'correct',\n",
       " 'could',\n",
       " 'country',\n",
       " 'counts',\n",
       " 'course',\n",
       " 'cousins',\n",
       " 'cows',\n",
       " 'cracked',\n",
       " 'crafty',\n",
       " 'cranky',\n",
       " 'crash',\n",
       " 'crashed',\n",
       " 'crazy',\n",
       " 'creative',\n",
       " 'credible',\n",
       " 'creepy',\n",
       " 'cried',\n",
       " 'crook',\n",
       " 'cruel',\n",
       " 'crushed',\n",
       " 'cry',\n",
       " 'crying',\n",
       " 'cultured',\n",
       " 'cured',\n",
       " 'curious',\n",
       " 'cut',\n",
       " 'cute',\n",
       " 'd',\n",
       " 'dad',\n",
       " 'damaged',\n",
       " 'dance',\n",
       " 'dancing',\n",
       " 'dangerous',\n",
       " 'dare',\n",
       " 'dark',\n",
       " 'date',\n",
       " 'dating',\n",
       " 'day',\n",
       " 'dead',\n",
       " 'deaf',\n",
       " 'decide',\n",
       " 'decided',\n",
       " 'decline',\n",
       " 'deep',\n",
       " 'dehydrated',\n",
       " 'demented',\n",
       " 'denied',\n",
       " 'deny',\n",
       " 'dependable',\n",
       " 'depressed',\n",
       " 'deserve',\n",
       " 'deserved',\n",
       " 'despair',\n",
       " 'desperate',\n",
       " 'despise',\n",
       " 'details',\n",
       " 'devastated',\n",
       " 'diabetes',\n",
       " 'diabetic',\n",
       " 'diary',\n",
       " 'did',\n",
       " 'die',\n",
       " 'died',\n",
       " 'dies',\n",
       " 'dieting',\n",
       " 'different',\n",
       " 'dirty',\n",
       " 'disagreed',\n",
       " 'discreet',\n",
       " 'disgust',\n",
       " 'disgusted',\n",
       " 'dishonest',\n",
       " 'disobeyed',\n",
       " 'divorced',\n",
       " 'dizzy',\n",
       " 'do',\n",
       " 'doable',\n",
       " 'doctor',\n",
       " 'doctors',\n",
       " 'does',\n",
       " 'dog',\n",
       " 'dogs',\n",
       " 'done',\n",
       " 'donut',\n",
       " 'doomed',\n",
       " 'door',\n",
       " 'doors',\n",
       " 'dope',\n",
       " 'doubt',\n",
       " 'down',\n",
       " 'downstairs',\n",
       " 'drag',\n",
       " 'drank',\n",
       " 'draws',\n",
       " 'dream',\n",
       " 'dreamer',\n",
       " 'dreaming',\n",
       " 'dreams',\n",
       " 'dressed',\n",
       " 'drink',\n",
       " 'drinks',\n",
       " 'drive',\n",
       " 'drives',\n",
       " 'driving',\n",
       " 'dropped',\n",
       " 'drowned',\n",
       " 'drowning',\n",
       " 'drunk',\n",
       " 'dump',\n",
       " 'duty',\n",
       " 'dying',\n",
       " 'early',\n",
       " 'ears',\n",
       " 'easy',\n",
       " 'eat',\n",
       " 'eaten',\n",
       " 'eating',\n",
       " 'eats',\n",
       " 'effort',\n",
       " 'eggs',\n",
       " 'eight',\n",
       " 'else',\n",
       " 'elusive',\n",
       " 'embarrassed',\n",
       " 'embraced',\n",
       " 'engaged',\n",
       " 'enjoy',\n",
       " 'enough',\n",
       " 'envious',\n",
       " 'envy',\n",
       " 'escaped',\n",
       " 'escaping',\n",
       " 'even',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'evidence',\n",
       " 'evident',\n",
       " 'evil',\n",
       " 'exaggerated',\n",
       " 'excited',\n",
       " 'exciting',\n",
       " 'exercised',\n",
       " 'exhausted',\n",
       " 'exist',\n",
       " 'expensive',\n",
       " 'explain',\n",
       " 'eyes',\n",
       " 'fail',\n",
       " 'failed',\n",
       " 'faint',\n",
       " 'fainted',\n",
       " 'fair',\n",
       " 'faithful',\n",
       " 'faking',\n",
       " 'falling',\n",
       " 'family',\n",
       " 'famished',\n",
       " 'famous',\n",
       " 'fan',\n",
       " 'fanatics',\n",
       " 'far',\n",
       " 'farsighted',\n",
       " 'fascinated',\n",
       " 'fast',\n",
       " 'faster',\n",
       " 'fasting',\n",
       " 'fat',\n",
       " 'fatal',\n",
       " 'father',\n",
       " 'fear',\n",
       " 'fearless',\n",
       " 'fed',\n",
       " 'feel',\n",
       " 'feels',\n",
       " 'fell',\n",
       " 'felt',\n",
       " 'feverish',\n",
       " 'fiasco',\n",
       " 'fight',\n",
       " 'fighting',\n",
       " 'filming',\n",
       " 'find',\n",
       " 'fine',\n",
       " 'finicky',\n",
       " 'finish',\n",
       " 'finished',\n",
       " 'fire',\n",
       " 'fired',\n",
       " 'first',\n",
       " 'fish',\n",
       " 'fishing',\n",
       " 'fit',\n",
       " 'fix',\n",
       " 'fixed',\n",
       " 'flabby',\n",
       " 'flag',\n",
       " 'flat',\n",
       " 'flies',\n",
       " 'flowers',\n",
       " 'fly',\n",
       " 'flying',\n",
       " 'follow',\n",
       " 'followed',\n",
       " 'food',\n",
       " 'fooled',\n",
       " 'foolish',\n",
       " 'foot',\n",
       " 'for',\n",
       " 'forbid',\n",
       " 'forbidden',\n",
       " 'forget',\n",
       " 'forgetful',\n",
       " 'forgive',\n",
       " 'forgot',\n",
       " 'forgotten',\n",
       " 'found',\n",
       " 'frantic',\n",
       " 'freaked',\n",
       " 'free',\n",
       " 'freezing',\n",
       " 'fret',\n",
       " 'friend',\n",
       " 'friendly',\n",
       " 'friends',\n",
       " 'fruit',\n",
       " 'full',\n",
       " 'fun',\n",
       " 'funny',\n",
       " 'furious',\n",
       " 'fussy',\n",
       " 'futon',\n",
       " 'gambling',\n",
       " 'garbage',\n",
       " 'gas',\n",
       " 'gate',\n",
       " 'gave',\n",
       " 'gear',\n",
       " 'gentle',\n",
       " 'genuine',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'girl',\n",
       " 'girls',\n",
       " 'give',\n",
       " 'glad',\n",
       " 'glasses',\n",
       " 'gloat',\n",
       " 'go',\n",
       " 'goes',\n",
       " 'going',\n",
       " 'golf',\n",
       " 'gone',\n",
       " 'good',\n",
       " 'gorgeous',\n",
       " 'got',\n",
       " 'grab',\n",
       " 'grateful',\n",
       " 'great',\n",
       " 'greedy',\n",
       " 'green',\n",
       " 'grew',\n",
       " 'gross',\n",
       " 'grounded',\n",
       " 'grow',\n",
       " 'grumbling',\n",
       " 'grumpy',\n",
       " 'guarantee',\n",
       " 'guess',\n",
       " 'guest',\n",
       " 'guests',\n",
       " 'guilty',\n",
       " 'gullible',\n",
       " 'gun',\n",
       " 'guts',\n",
       " 'guy',\n",
       " 'guys',\n",
       " 'had',\n",
       " 'ham',\n",
       " 'hand',\n",
       " 'handle',\n",
       " 'handled',\n",
       " 'happen',\n",
       " 'happens',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'hardly',\n",
       " 'hardworking',\n",
       " 'has',\n",
       " 'hassle',\n",
       " 'hate',\n",
       " 'hated',\n",
       " 'hates',\n",
       " 'have',\n",
       " 'he',\n",
       " 'heal',\n",
       " 'heap',\n",
       " 'hear',\n",
       " 'heel',\n",
       " 'hello',\n",
       " 'help',\n",
       " 'helped',\n",
       " 'helpful',\n",
       " 'helping',\n",
       " 'helpless',\n",
       " 'helps',\n",
       " 'henpecked',\n",
       " 'her',\n",
       " 'here',\n",
       " 'heroic',\n",
       " 'hers',\n",
       " 'hesitated',\n",
       " 'hideous',\n",
       " 'hiding',\n",
       " 'high',\n",
       " 'him',\n",
       " 'hired',\n",
       " 'hiring',\n",
       " 'his',\n",
       " 'hit',\n",
       " 'hole',\n",
       " 'home',\n",
       " 'honest',\n",
       " 'honor',\n",
       " 'honored',\n",
       " 'hope',\n",
       " 'horrible',\n",
       " 'horrified',\n",
       " 'horse',\n",
       " 'hot',\n",
       " 'house',\n",
       " 'huge',\n",
       " 'hugged',\n",
       " 'human',\n",
       " 'humble',\n",
       " 'humming',\n",
       " 'hung',\n",
       " 'hungry',\n",
       " 'hurry',\n",
       " 'hurt',\n",
       " 'hurts',\n",
       " 'ice',\n",
       " 'icky',\n",
       " 'idea',\n",
       " 'idiot',\n",
       " 'idiots',\n",
       " 'if',\n",
       " 'ignore',\n",
       " 'ignored',\n",
       " 'ill',\n",
       " 'illiterate',\n",
       " 'immune',\n",
       " 'impatient',\n",
       " 'important',\n",
       " 'impressed',\n",
       " 'improvised',\n",
       " 'impulsive',\n",
       " 'in',\n",
       " 'included',\n",
       " 'innocent',\n",
       " 'insane',\n",
       " 'insects',\n",
       " 'inside',\n",
       " 'insist',\n",
       " 'inspiring',\n",
       " 'insured',\n",
       " 'interested',\n",
       " 'interfering',\n",
       " 'interrupt',\n",
       " 'intrigued',\n",
       " 'intrigues',\n",
       " 'introverted',\n",
       " 'invited',\n",
       " 'involved',\n",
       " 'iron',\n",
       " 'ironic',\n",
       " 'is',\n",
       " 'isolated',\n",
       " 'it',\n",
       " 'itches',\n",
       " 'jealous',\n",
       " 'jerk',\n",
       " 'job',\n",
       " 'jobs',\n",
       " 'jogging',\n",
       " 'join',\n",
       " 'joke',\n",
       " 'joking',\n",
       " 'juggle',\n",
       " 'jump',\n",
       " 'jumped',\n",
       " 'just',\n",
       " 'justice',\n",
       " 'karaoke',\n",
       " 'keep',\n",
       " 'keeps',\n",
       " 'kept',\n",
       " 'key',\n",
       " 'kidding',\n",
       " 'kids',\n",
       " 'kill',\n",
       " 'killed',\n",
       " 'kind',\n",
       " 'kiss',\n",
       " 'kissed',\n",
       " 'knew',\n",
       " 'knits',\n",
       " 'know',\n",
       " 'knows',\n",
       " 'lame',\n",
       " 'last',\n",
       " 'late',\n",
       " 'later',\n",
       " 'laugh',\n",
       " 'laughed',\n",
       " 'laughing',\n",
       " 'laughs',\n",
       " 'law',\n",
       " 'lawyer',\n",
       " 'lawyers',\n",
       " ...]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
